{
  "skill_id": "write_tests",
  "version": "1.0.0",
  "description": "Generate comprehensive test suite with 90% token reduction",
  "token_cost": 1000,
  "decision_tree": {
    "question": "What type of tests do you need?",
    "options": {
      "unit_test": {
        "description": "Unit tests for individual functions/classes",
        "template": "pytest_unit.py.jinja2",
        "file_path": "backend/tests/test_{{ module_name }}.py",
        "extension": ".py"
      },
      "integration_test": {
        "description": "Integration tests for API endpoints",
        "template": "pytest_integration.py.jinja2",
        "file_path": "backend/tests/test_{{ module_name }}_integration.py",
        "extension": ".py"
      },
      "streaming_test": {
        "description": "Tests for streaming/WebSocket endpoints",
        "template": "pytest_streaming.py.jinja2",
        "file_path": "backend/tests/test_{{ module_name }}_streaming.py",
        "extension": ".py"
      },
      "agent_test": {
        "description": "Tests for LangGraph agents",
        "template": "pytest_agent.py.jinja2",
        "file_path": "backend/tests/test_{{ module_name }}_agent.py",
        "extension": ".py"
      }
    }
  },
  "prerequisites": {
    "files_exist": [
      "backend/tests/",
      "backend/app/"
    ],
    "env_vars": [
      "DATABASE_URL",
      "REDIS_URL"
    ]
  },
  "code_templates": {
    "unit_test": "import pytest\nfrom unittest.mock import Mock, patch, AsyncMock\nfrom datetime import datetime\n\nfrom app.services.{{ module_name }} import {{ ClassName }}\nfrom app.schemas.{{ schema_name }} import {{ SchemaName }}Create, {{ SchemaName }}Response\nfrom app.models.{{ model_name }} import {{ ModelName }}\n\n\nclass Test{{ ClassName }}:\n    \"\"\"Unit tests for {{ ClassName }}.\"\"\"\n    \n    @pytest.fixture\n    def {{ instance_name }}(self):\n        \"\"\"Create {{ instance_name }} instance for testing.\"\"\"\n        return {{ ClassName }}()\n    \n    @pytest.fixture\n    def sample_data(self):\n        \"\"\"Sample data for testing.\"\"\"\n        return {\n            \"name\": \"Test {{ name }}\",\n            \"description\": \"Test description\",\n            \"status\": \"active\"\n        }\n    \n    def test_{{ method_name }}_success(self, {{ instance_name }}, sample_data):\n        \"\"\"Test successful {{ method_name }} execution.\"\"\"\n        # Arrange\n        expected_result = {\n            \"id\": 1,\n            \"name\": \"Test {{ name }}\",\n            \"status\": \"completed\"\n        }\n        \n        # Act\n        with patch.object({{ instance_name }}, '_{{ private_method }}', return_value=expected_result):\n            result = {{ instance_name }}.{{ method_name }}(sample_data)\n        \n        # Assert\n        assert result == expected_result\n        assert result[\"status\"] == \"completed\"\n    \n    def test_{{ method_name }}_validation_error(self, {{ instance_name }}):\n        \"\"\"Test {{ method_name }} with invalid data.\"\"\"\n        # Arrange\n        invalid_data = {\n            \"name\": \"\",  # Invalid: empty name\n            \"description\": \"Test description\"\n        }\n        \n        # Act & Assert\n        with pytest.raises(ValueError, match=\"Name cannot be empty\"):\n            {{ instance_name }}.{{ method_name }}(invalid_data)\n    \n    def test_{{ method_name }}_database_error(self, {{ instance_name }}, sample_data):\n        \"\"\"Test {{ method_name }} with database error.\"\"\"\n        # Arrange\n        with patch.object({{ instance_name }}, '_{{ private_method }}', side_effect=Exception(\"Database error\")):\n            # Act & Assert\n            with pytest.raises(Exception, match=\"Database error\"):\n                {{ instance_name }}.{{ method_name }}(sample_data)\n    \n    @pytest.mark.asyncio\n    async def test_async_{{ method_name }}(self, {{ instance_name }}, sample_data):\n        \"\"\"Test async {{ method_name }} execution.\"\"\"\n        # Arrange\n        expected_result = {\"status\": \"completed\"}\n        \n        # Act\n        with patch.object({{ instance_name }}, '_{{ private_method }}', new_callable=AsyncMock, return_value=expected_result):\n            result = await {{ instance_name }}.async_{{ method_name }}(sample_data)\n        \n        # Assert\n        assert result == expected_result\n    \n    def test_{{ method_name }}_with_mocks(self, {{ instance_name }}):\n        \"\"\"Test {{ method_name }} with various mocks.\"\"\"\n        # Arrange\n        with patch('app.services.{{ module_name }}.external_service') as mock_service, \\\n             patch('app.services.{{ module_name }}.logger') as mock_logger:\n            \n            mock_service.process.return_value = {\"processed\": True}\n            \n            # Act\n            result = {{ instance_name }}.{{ method_name }}({\"test\": \"data\"})\n            \n            # Assert\n            mock_service.process.assert_called_once_with({\"test\": \"data\"})\n            mock_logger.info.assert_called()\n            assert result[\"processed\"] is True",
    "integration_test": "import pytest\nfrom fastapi.testclient import TestClient\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom unittest.mock import patch\n\nfrom app.main import app\nfrom app.models.database import get_db, Base\nfrom app.models.{{ model_name }} import {{ ModelName }}\nfrom app.schemas.{{ schema_name }} import {{ SchemaName }}Create\n\n# Test database\nSQLALCHEMY_DATABASE_URL = \"sqlite:///./test.db\"\nengine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False})\nTestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\n@pytest.fixture(scope=\"module\")\ndef test_db():\n    \"\"\"Create test database.\"\"\"\n    Base.metadata.create_all(bind=engine)\n    yield\n    Base.metadata.drop_all(bind=engine)\n\n@pytest.fixture(scope=\"module\")\ndef db_session(test_db):\n    \"\"\"Create database session for testing.\"\"\"\n    connection = engine.connect()\n    transaction = connection.begin()\n    session = TestingSessionLocal(bind=connection)\n    yield session\n    session.close()\n    transaction.rollback()\n    connection.close()\n\n@pytest.fixture(scope=\"module\")\ndef client(db_session):\n    \"\"\"Create test client.\"\"\"\n    def override_get_db():\n        yield db_session\n    \n    app.dependency_overrides[get_db] = override_get_db\n    with TestClient(app) as test_client:\n        yield test_client\n    app.dependency_overrides.clear()\n\n\nclass Test{{ ClassName }}API:\n    \"\"\"Integration tests for {{ ClassName }} API endpoints.\"\"\"\n    \n    def test_create_{{ name }}_success(self, client, db_session):\n        \"\"\"Test successful {{ name }} creation.\"\"\"\n        # Arrange\n        {{ name }}_data = {\n            \"name\": \"Test {{ name }}\",\n            \"description\": \"Test description\",\n            \"status\": \"active\"\n        }\n        \n        # Act\n        response = client.post(\"/api/v1/{{ name }}s/\", json={{ name }}_data)\n        \n        # Assert\n        assert response.status_code == 201\n        data = response.json()\n        assert data[\"name\"] == \"Test {{ name }}\"\n        assert data[\"status\"] == \"active\"\n        assert \"id\" in data\n    \n    def test_create_{{ name }}_validation_error(self, client):\n        \"\"\"Test {{ name }} creation with validation error.\"\"\"\n        # Arrange\n        invalid_data = {\n            \"name\": \"\",  # Invalid: empty name\n            \"description\": \"Test description\"\n        }\n        \n        # Act\n        response = client.post(\"/api/v1/{{ name }}s/\", json=invalid_data)\n        \n        # Assert\n        assert response.status_code == 422\n        assert \"validation error\" in response.json()[\"detail\"][0][\"msg\"].lower()\n    \n    def test_get_{{ name }}_success(self, client, db_session):\n        \"\"\"Test successful {{ name }} retrieval.\"\"\"\n        # Arrange - Create a {{ name }} first\n        {{ name }}_data = {\n            \"name\": \"Test {{ name }}\",\n            \"description\": \"Test description\",\n            \"status\": \"active\"\n        }\n        create_response = client.post(\"/api/v1/{{ name }}s/\", json={{ name }}_data)\n        {{ name }}_id = create_response.json()[\"id\"]\n        \n        # Act\n        response = client.get(f\"/api/v1/{{ name }}s/{ {{ name }}_id}\")\n        \n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"id\"] == {{ name }}_id\n        assert data[\"name\"] == \"Test {{ name }}\"\n    \n    def test_get_{{ name }}_not_found(self, client):\n        \"\"\"Test {{ name }} retrieval with non-existent ID.\"\"\"\n        # Act\n        response = client.get(\"/api/v1/{{ name }}s/999999\")\n        \n        # Assert\n        assert response.status_code == 404\n        assert \"not found\" in response.json()[\"detail\"].lower()\n    \n    def test_list_{{ name }}s_success(self, client, db_session):\n        \"\"\"Test successful {{ name }}s listing.\"\"\"\n        # Arrange - Create multiple {{ name }}s\n        for i in range(3):\n            {{ name }}_data = {\n                \"name\": f\"Test {{ name }} {i}\",\n                \"description\": f\"Test description {i}\",\n                \"status\": \"active\"\n            }\n            client.post(\"/api/v1/{{ name }}s/\", json={{ name }}_data)\n        \n        # Act\n        response = client.get(\"/api/v1/{{ name }}s/\")\n        \n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert len(data) >= 3\n        assert all(\"id\" in item for item in data)\n    \n    def test_update_{{ name }}_success(self, client, db_session):\n        \"\"\"Test successful {{ name }} update.\"\"\"\n        # Arrange - Create a {{ name }} first\n        {{ name }}_data = {\n            \"name\": \"Test {{ name }}\",\n            \"description\": \"Test description\",\n            \"status\": \"active\"\n        }\n        create_response = client.post(\"/api/v1/{{ name }}s/\", json={{ name }}_data)\n        {{ name }}_id = create_response.json()[\"id\"]\n        \n        update_data = {\n            \"name\": \"Updated {{ name }}\",\n            \"status\": \"inactive\"\n        }\n        \n        # Act\n        response = client.put(f\"/api/v1/{{ name }}s/{ {{ name }}_id}\", json=update_data)\n        \n        # Assert\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"name\"] == \"Updated {{ name }}\"\n        assert data[\"status\"] == \"inactive\"\n    \n    def test_delete_{{ name }}_success(self, client, db_session):\n        \"\"\"Test successful {{ name }} deletion.\"\"\"\n        # Arrange - Create a {{ name }} first\n        {{ name }}_data = {\n            \"name\": \"Test {{ name }}\",\n            \"description\": \"Test description\",\n            \"status\": \"active\"\n        }\n        create_response = client.post(\"/api/v1/{{ name }}s/\", json={{ name }}_data)\n        {{ name }}_id = create_response.json()[\"id\"]\n        \n        # Act\n        response = client.delete(f\"/api/v1/{{ name }}s/{ {{ name }}_id}\")\n        \n        # Assert\n        assert response.status_code == 204\n        \n        # Verify deletion\n        get_response = client.get(f\"/api/v1/{{ name }}s/{ {{ name }}_id}\")\n        assert get_response.status_code == 404",
    "streaming_test": "import pytest\nimport asyncio\nimport json\nfrom fastapi.testclient import TestClient\nfrom unittest.mock import patch, AsyncMock\n\nfrom app.main import app\nfrom app.schemas.{{ schema_name }} import {{ SchemaName }}Request\n\n\nclass Test{{ ClassName }}Streaming:\n    \"\"\"Tests for {{ ClassName }} streaming endpoints.\"\"\"\n    \n    @pytest.fixture\n    def client(self):\n        \"\"\"Create test client.\"\"\"\n        return TestClient(app)\n    \n    def test_stream_{{ name }}_success(self, client):\n        \"\"\"Test successful {{ name }} streaming.\"\"\"\n        # Arrange\n        request_data = {\n            \"input\": \"Test input\",\n            \"options\": {\"stream\": True}\n        }\n        \n        # Act\n        with patch('app.services.{{ module_name }}.{{ ClassName }}.process_stream') as mock_stream:\n            mock_stream.return_value = [\n                {\"type\": \"start\", \"message\": \"Processing started\"},\n                {\"type\": \"progress\", \"message\": \"Processing...\", \"percent\": 50},\n                {\"type\": \"result\", \"data\": {\"result\": \"success\"}},\n                {\"type\": \"complete\", \"message\": \"Processing completed\"}\n            ]\n            \n            response = client.post(\"/api/v1/{{ name }}s/stream\", json=request_data)\n        \n        # Assert\n        assert response.status_code == 200\n        assert response.headers[\"content-type\"] == \"text/plain; charset=utf-8\"\n        \n        # Parse streaming response\n        content = response.text\n        lines = [line for line in content.split('\\n') if line.strip()]\n        \n        assert len(lines) >= 4\n        assert \"data: {\\\"type\\\": \\\"start\\\"\" in content\n        assert \"data: {\\\"type\\\": \\\"complete\\\"\" in content\n    \n    def test_stream_{{ name }}_error(self, client):\n        \"\"\"Test {{ name }} streaming with error.\"\"\"\n        # Arrange\n        request_data = {\n            \"input\": \"Invalid input\",\n            \"options\": {\"stream\": True}\n        }\n        \n        # Act\n        with patch('app.services.{{ module_name }}.{{ ClassName }}.process_stream') as mock_stream:\n            mock_stream.side_effect = Exception(\"Processing error\")\n            \n            response = client.post(\"/api/v1/{{ name }}s/stream\", json=request_data)\n        \n        # Assert\n        assert response.status_code == 200  # Streaming endpoint should return 200 even on error\n        assert \"error\" in response.text.lower()\n    \n    @pytest.mark.asyncio\n    async def test_websocket_{{ name }}(self, client):\n        \"\"\"Test {{ name }} WebSocket connection.\"\"\"\n        # Arrange\n        test_message = {\n            \"type\": \"request\",\n            \"data\": {\"input\": \"Test input\"}\n        }\n        \n        # Act\n        with client.websocket_connect(\"/api/v1/{{ name }}s/ws\") as websocket:\n            # Send test message\n            websocket.send_text(json.dumps(test_message))\n            \n            # Receive response\n            response = websocket.receive_text()\n            data = json.loads(response)\n        \n        # Assert\n        assert data[\"type\"] == \"response\"\n        assert \"data\" in data\n        assert \"timestamp\" in data\n    \n    def test_stream_{{ name }}_validation_error(self, client):\n        \"\"\"Test {{ name }} streaming with validation error.\"\"\"\n        # Arrange\n        invalid_data = {\n            \"input\": \"\",  # Invalid: empty input\n            \"options\": {\"stream\": True}\n        }\n        \n        # Act\n        response = client.post(\"/api/v1/{{ name }}s/stream\", json=invalid_data)\n        \n        # Assert\n        assert response.status_code == 422\n        assert \"validation error\" in response.json()[\"detail\"][0][\"msg\"].lower()\n    \n    def test_stream_{{ name }}_timeout(self, client):\n        \"\"\"Test {{ name }} streaming timeout.\"\"\"\n        # Arrange\n        request_data = {\n            \"input\": \"Test input\",\n            \"options\": {\"stream\": True, \"timeout\": 1}\n        }\n        \n        # Act\n        with patch('app.services.{{ module_name }}.{{ ClassName }}.process_stream') as mock_stream:\n            async def slow_stream():\n                yield {\"type\": \"start\", \"message\": \"Processing started\"}\n                await asyncio.sleep(2)  # Simulate timeout\n                yield {\"type\": \"complete\", \"message\": \"Processing completed\"}\n            \n            mock_stream.return_value = slow_stream()\n            \n            response = client.post(\"/api/v1/{{ name }}s/stream\", json=request_data)\n        \n        # Assert\n        assert response.status_code == 200\n        # Should receive at least the start message before timeout\n        assert \"start\" in response.text",
    "agent_test": "import pytest\nfrom unittest.mock import Mock, patch, AsyncMock\nfrom datetime import datetime\n\nfrom app.services.langgraph.agents.{{ agent_name }}_agent import {{ AgentClassName }}\nfrom app.services.langgraph.schemas import {{ AgentState }}\nfrom app.services.langgraph.redis_checkpointer import get_redis_checkpointer\n\n\nclass Test{{ AgentClassName }}:\n    \"\"\"Tests for {{ AgentClassName }} LangGraph agent.\"\"\"\n    \n    @pytest.fixture\n    def agent(self):\n        \"\"\"Create agent instance for testing.\"\"\"\n        return {{ AgentClassName }}()\n    \n    @pytest.fixture\n    def sample_input(self):\n        \"\"\"Sample input data for testing.\"\"\"\n        return {\n            \"company_name\": \"Test Corp\",\n            \"industry\": \"SaaS\",\n            \"contact_email\": \"test@testcorp.com\"\n        }\n    \n    @pytest.fixture\n    def mock_checkpointer(self):\n        \"\"\"Mock Redis checkpointer.\"\"\"\n        with patch('app.services.langgraph.redis_checkpointer.get_redis_checkpointer') as mock:\n            mock_checkpointer = Mock()\n            mock_checkpointer.aget = AsyncMock(return_value=None)\n            mock_checkpointer.aset = AsyncMock(return_value=None)\n            mock.return_value = mock_checkpointer\n            yield mock_checkpointer\n    \n    def test_agent_initialization(self, agent):\n        \"\"\"Test agent initialization.\"\"\"\n        assert agent is not None\n        assert hasattr(agent, 'llm')\n        assert hasattr(agent, 'tools')\n    \n    def test_create_graph(self, agent):\n        \"\"\"Test graph creation.\"\"\"\n        graph = agent.create_graph()\n        assert graph is not None\n        # Verify graph has expected nodes\n        assert hasattr(graph, 'nodes')\n    \n    @pytest.mark.asyncio\n    async def test_execute_success(self, agent, sample_input, mock_checkpointer):\n        \"\"\"Test successful agent execution.\"\"\"\n        # Arrange\n        expected_result = {\n            \"result\": {\"score\": 85, \"reasoning\": \"Good fit\"},\n            \"agent_type\": \"{{ agent_name }}\",\n            \"status\": \"completed\"\n        }\n        \n        # Mock the graph execution\n        with patch.object(agent, 'create_graph') as mock_create_graph:\n            mock_graph = Mock()\n            mock_graph.ainvoke = AsyncMock(return_value=expected_result)\n            mock_create_graph.return_value = mock_graph\n            \n            # Act\n            result = await agent.execute(sample_input)\n        \n        # Assert\n        assert result == expected_result\n        assert result[\"status\"] == \"completed\"\n        assert result[\"agent_type\"] == \"{{ agent_name }}\"\n    \n    @pytest.mark.asyncio\n    async def test_execute_with_streaming(self, agent, sample_input, mock_checkpointer):\n        \"\"\"Test agent execution with streaming.\"\"\"\n        # Arrange\n        stream_chunks = [\n            {\"type\": \"start\", \"message\": \"Processing started\"},\n            {\"type\": \"progress\", \"message\": \"Analyzing data...\"},\n            {\"type\": \"result\", \"data\": {\"score\": 85}},\n            {\"type\": \"complete\", \"message\": \"Processing completed\"}\n        ]\n        \n        # Mock the graph execution\n        with patch.object(agent, 'create_graph') as mock_create_graph:\n            mock_graph = Mock()\n            \n            async def mock_astream(input_data):\n                for chunk in stream_chunks:\n                    yield chunk\n            \n            mock_graph.astream = mock_astream\n            mock_create_graph.return_value = mock_graph\n            \n            # Act\n            chunks = []\n            async for chunk in agent.execute_stream(sample_input):\n                chunks.append(chunk)\n        \n        # Assert\n        assert len(chunks) == 4\n        assert chunks[0][\"type\"] == \"start\"\n        assert chunks[-1][\"type\"] == \"complete\"\n    \n    @pytest.mark.asyncio\n    async def test_execute_error(self, agent, sample_input, mock_checkpointer):\n        \"\"\"Test agent execution with error.\"\"\"\n        # Arrange\n        with patch.object(agent, 'create_graph') as mock_create_graph:\n            mock_graph = Mock()\n            mock_graph.ainvoke = AsyncMock(side_effect=Exception(\"Agent error\"))\n            mock_create_graph.return_value = mock_graph\n            \n            # Act & Assert\n            with pytest.raises(Exception, match=\"Agent error\"):\n                await agent.execute(sample_input)\n    \n    def test_agent_state_validation(self, agent):\n        \"\"\"Test agent state validation.\"\"\"\n        # Valid state\n        valid_state = {{ AgentState }}(\n            messages=[],\n            current_step=\"started\",\n            confidence=0.0,\n            result={}\n        )\n        assert valid_state is not None\n        \n        # Invalid state should raise validation error\n        with pytest.raises(Exception):\n            {{ AgentState }}(\n                messages=[],\n                current_step=\"invalid_step\",\n                confidence=1.5,  # Invalid: confidence > 1.0\n                result={}\n            )\n    \n    @pytest.mark.asyncio\n    async def test_agent_with_tools(self, agent, sample_input):\n        \"\"\"Test agent execution with tools.\"\"\"\n        # Mock tools\n        mock_tool = Mock()\n        mock_tool.ainvoke = AsyncMock(return_value={\"tool_result\": \"success\"})\n        \n        with patch.object(agent, 'tools', [mock_tool]):\n            # Act\n            result = await agent.execute(sample_input)\n        \n        # Assert\n        assert result is not None\n        # Verify tool was called if applicable\n        # (This depends on the specific agent implementation)\n    \n    def test_agent_performance(self, agent, sample_input):\n        \"\"\"Test agent performance metrics.\"\"\"\n        import time\n        \n        # Measure execution time\n        start_time = time.time()\n        \n        # Mock fast execution\n        with patch.object(agent, 'create_graph') as mock_create_graph:\n            mock_graph = Mock()\n            mock_graph.ainvoke = AsyncMock(return_value={\"status\": \"completed\"})\n            mock_create_graph.return_value = mock_graph\n            \n            # Act\n            import asyncio\n            result = asyncio.run(agent.execute(sample_input))\n        \n        end_time = time.time()\n        execution_time = end_time - start_time\n        \n        # Assert performance (should be fast with mocks)\n        assert execution_time < 1.0  # Should complete in under 1 second\n        assert result[\"status\"] == \"completed\""
  },
  "related_skills": [
    "langgraph_agent",
    "fastapi_endpoint"
  ],
  "examples": [
    {
      "scenario": "Unit tests for lead service",
      "pattern": "unit_test",
      "parameters": {
        "module_name": "lead",
        "ClassName": "LeadService",
        "class_name": "LeadService",
        "instance_name": "lead_service",
        "method_name": "qualify_lead",
        "private_method": "process_qualification",
        "name": "lead",
        "schema_name": "lead",
        "SchemaName": "Lead",
        "model_name": "lead",
        "ModelName": "Lead"
      },
      "files_created": ["backend/tests/test_lead.py"],
      "test_command": "pytest backend/tests/test_lead.py -v"
    },
    {
      "scenario": "Integration tests for API",
      "pattern": "integration_test",
      "parameters": {
        "module_name": "leads",
        "ClassName": "LeadsAPI",
        "name": "lead",
        "schema_name": "lead",
        "SchemaName": "Lead",
        "model_name": "lead",
        "ModelName": "Lead"
      },
      "files_created": ["backend/tests/test_leads_integration.py"],
      "test_command": "pytest backend/tests/test_leads_integration.py -v"
    }
  ]
}