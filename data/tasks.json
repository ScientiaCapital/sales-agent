{
  "tasks": [
    {
      "id": "ba4bf981-b5f3-4883-93b5-ea4e24386738",
      "name": "Phase 2A: Implement Iterative Refinement Engine",
      "description": "Create 4-step iterative refinement loop (reflect → elaborate → critique → refine) to improve agent output quality by 40% with acceptable 4x cost increase. Integrates with BaseAgent pattern and WebSocket streaming.",
      "notes": "CRITICAL: Total latency must stay <2500ms. Use Cerebras llama3.1-8b for steps 1,2,4 (633ms each). Use Qwen-3-32B for critique step 3 (500ms). Track all 4 API calls in iterative_refinement_logs for cost monitoring.",
      "status": "pending",
      "dependencies": [],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "backend/app/services/base_agent.py",
          "type": "TO_MODIFY",
          "description": "Add iterate_refine() method",
          "lineStart": 50,
          "lineEnd": 289
        },
        {
          "path": "backend/app/services/iterative_refinement.py",
          "type": "CREATE",
          "description": "New iterative refinement service"
        },
        {
          "path": "backend/app/models/agent_models.py",
          "type": "TO_MODIFY",
          "description": "Add IterativeRefinementLog model"
        },
        {
          "path": "backend/alembic/versions/",
          "type": "CREATE",
          "description": "Migration for refinement_logs table"
        }
      ],
      "implementationGuide": "1. Create backend/app/services/iterative_refinement.py with IterativeRefinementService class\n2. Implement 4 methods: _reflect() extracts key attributes (200ms, llama3.1-8b), _elaborate() initial generation (633ms), _critique() using Qwen-3-32B (500ms), _refine() final output (633ms)\n3. Add iterate_refine() async method to BaseAgent class: accepts input_data, returns refined result with metadata\n4. Create Alembic migration: iterative_refinement_logs table (id, agent_execution_id FK, iteration_number, model_used, input_prompt, output_text, latency_ms, cost_usd, created_at)\n5. Alter agent_executions table: ADD COLUMN refinement_iterations INTEGER DEFAULT 1\n6. WebSocket streaming: Emit 'iteration_update' events for each step with step_name, content, latency_ms\n7. Integration: QualificationAgent calls iterate_refine() for leads with score >70\n8. Cost tracking: Log each of 4 Cerebras API calls with cumulative cost calculation",
      "verificationCriteria": "1. All 4 refinement steps complete in <2500ms total\n2. Each step logged in iterative_refinement_logs table\n3. QualificationAgent produces 40% higher quality scores for leads >70\n4. WebSocket clients receive 4 iteration_update events\n5. Cost tracking shows 4x increase ($0.000024 vs $0.000006)\n6. Tests in test_iterative_refinement.py achieve 95% coverage\n7. Backward compatible: Agents without refinement still work",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "92cc5fb6-8dd7-4938-8b27-5615738681ac",
      "name": "Phase 2A: Frontend Iterative Refinement Visualization",
      "description": "Create React components and WebSocket hooks to display real-time iterative refinement progress with 4-step visual flow and performance metrics.",
      "notes": "Use existing WebSocket infrastructure. Ensure smooth animations when transitioning between steps. Display costs in USD with 6 decimal precision.",
      "status": "pending",
      "dependencies": [
        {
          "taskId": "ba4bf981-b5f3-4883-93b5-ea4e24386738"
        }
      ],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "frontend/src/hooks/useIterativeRefinement.ts",
          "type": "CREATE",
          "description": "WebSocket hook for refinement streaming"
        },
        {
          "path": "frontend/src/components/IterativeRefinement.tsx",
          "type": "CREATE",
          "description": "4-step refinement UI component"
        },
        {
          "path": "frontend/src/App.tsx",
          "type": "TO_MODIFY",
          "description": "Add /leads/:id/refinement route"
        }
      ],
      "implementationGuide": "1. Create frontend/src/hooks/useIterativeRefinement.ts:\n   - WebSocket connection to ws://localhost:8001/ws/stream/{streamId}\n   - Listen for 'iteration_update' events\n   - State management: iterations[], currentIteration, progress percentage\n   - Return { iterations, currentIteration, progress: (current/4)*100 }\n\n2. Create frontend/src/components/IterativeRefinement.tsx:\n   - Display 4-step flow: Reflect → Elaborate → Critique → Refine\n   - Progress bar showing completion percentage\n   - Each step card shows: step name, content preview, latency, status (pending/active/complete)\n   - Highlight current step in blue, completed in green\n   - Show total cost and latency at bottom\n\n3. Add route /leads/:id/refinement to display refinement history\n4. Integrate with existing lead detail page - show refinement toggle\n5. Tailwind CSS styling with smooth animations for step transitions",
      "verificationCriteria": "1. WebSocket receives all 4 iteration_update events\n2. Progress bar accurately reflects completion (0% → 25% → 50% → 75% → 100%)\n3. Each step displays content preview (max 200 chars)\n4. Total latency and cost displayed correctly\n5. Smooth animations between steps (300ms transition)\n6. Mobile responsive design",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "adeee671-4c9e-44b9-90c2-f0616c72c1dd",
      "name": "Phase 2B: Build Multi-Agent Research Pipeline",
      "description": "Create 5-agent assembly line for web research: QueryGenerator → WebSearcher → Summarizer → Synthesizer → Formatter. Integrates Exa API for web search and applies iterative refinement to article summaries.",
      "notes": "CRITICAL: Add exa-py to requirements.txt. Set EXA_API_KEY in .env. Total pipeline latency target: <10 seconds. Use asyncio.gather() for parallel article summarization. Store all execution details in research_pipeline_executions table.",
      "status": "pending",
      "dependencies": [
        {
          "taskId": "ba4bf981-b5f3-4883-93b5-ea4e24386738"
        }
      ],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "backend/app/services/research/query_generator.py",
          "type": "CREATE",
          "description": "Query generation agent"
        },
        {
          "path": "backend/app/services/research/web_searcher.py",
          "type": "CREATE",
          "description": "Exa API web search integration"
        },
        {
          "path": "backend/app/services/research/summarizer.py",
          "type": "CREATE",
          "description": "Article summarization with refinement"
        },
        {
          "path": "backend/app/services/research/synthesizer.py",
          "type": "CREATE",
          "description": "Multi-article synthesis"
        },
        {
          "path": "backend/app/services/research/formatter.py",
          "type": "CREATE",
          "description": "Output formatting"
        },
        {
          "path": "backend/requirements.txt",
          "type": "TO_MODIFY",
          "description": "Add exa-py>=1.0.0"
        },
        {
          "path": ".env",
          "type": "REFERENCE",
          "description": "Add EXA_API_KEY environment variable"
        }
      ],
      "implementationGuide": "1. Create backend/app/services/research/ directory\n\n2. query_generator.py:\n   - Analyze lead_data to extract research needs\n   - Generate focused search query using Cerebras (200ms)\n   - Return: {\"query\": str, \"focus_areas\": [str]}\n\n3. web_searcher.py:\n   - Integrate Exa API (pip install exa-py)\n   - Execute search with generated query\n   - Return: [{\"url\": str, \"title\": str, \"content\": str}] (max 10 articles)\n\n4. summarizer.py:\n   - For each article, apply iterative refinement:\n     * Reflect: Extract key points\n     * Elaborate: v1 summary\n     * Critique: Quality check\n     * Refine: Final summary\n   - Parallel processing for multiple articles\n   - Return: [{\"article_id\": str, \"summary\": str, \"key_points\": [str]}]\n\n5. synthesizer.py:\n   - Combine all article summaries into coherent report\n   - Use Cerebras for synthesis (633ms)\n   - Return: {\"synthesis\": str, \"sources\": [str]}\n\n6. formatter.py:\n   - Format final output for enrichment_results table\n   - Add citations, structure sections\n   - Return: {\"formatted_report\": str, \"metadata\": dict}\n\n7. Create ResearchAgent(BaseAgent) that orchestrates all 5 steps\n8. Alembic migration: research_pipeline_executions table\n9. API endpoint: POST /api/research/execute",
      "verificationCriteria": "1. Complete pipeline executes in <10 seconds\n2. QueryGenerator produces focused search query from lead data\n3. WebSearcher returns 5-10 relevant articles via Exa API\n4. Summarizer applies iterative refinement to each article\n5. Synthesizer combines summaries into coherent report\n6. Formatter structures output with citations\n7. All steps logged in research_pipeline_executions table\n8. Tests achieve 92% coverage",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "09810281-dd8b-439d-9f4d-eb8f98ea43a3",
      "name": "Phase 2B: Implement Agent Transfer System",
      "description": "Create function-based agent transfer system enabling intelligent handoffs between specialized agents (QualificationAgent → EnrichmentAgent → BDRAgent) with context preservation and history tracking.",
      "notes": "CRITICAL: Transfers must preserve all context in JSONB field. Use function_tool pattern from LiveKit tutorial. Maximum 5 transfers per workflow to prevent loops. Track transfer count in workflow_state.",
      "status": "pending",
      "dependencies": [],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "backend/app/services/agent_transfer.py",
          "type": "CREATE",
          "description": "Agent transfer orchestration"
        },
        {
          "path": "backend/app/services/base_agent.py",
          "type": "TO_MODIFY",
          "description": "Add get_transfer_tools() and transfer_to() methods"
        },
        {
          "path": "backend/app/models/agent_models.py",
          "type": "TO_MODIFY",
          "description": "Add AgentTransferHistory model"
        },
        {
          "path": "backend/alembic/versions/",
          "type": "CREATE",
          "description": "Migration for agent_transfer_history"
        }
      ],
      "implementationGuide": "1. Create backend/app/services/agent_transfer.py:\n\n   TransferTool class:\n   - @dataclass with target_agent: str, context: Dict, reason: str\n   - validate() method ensures target exists\n   \n   AgentTransferManager:\n   - transfer_to(from_agent, to_agent, context) async method\n   - Logs transfer in agent_transfer_history table\n   - Updates workflow_state.current_agent_type\n   - Returns new agent instance with injected context\n\n2. Extend BaseAgent class:\n   - Add abstract method get_transfer_tools() → List[TransferTool]\n   - Add async transfer_to(target: str) method\n   - Transfers preserve: lead_id, previous_scores, conversation_history\n\n3. Implement in concrete agents:\n   QualificationAgent.get_transfer_tools():\n     - If score >70: Can transfer to EnrichmentAgent or BDRAgent\n     - If score <40: Can transfer to MarketingAgent\n   \n   EnrichmentAgent.get_transfer_tools():\n     - After enrichment: Can transfer to BDRAgent or GrowthAgent\n\n4. Alembic migration:\n   - agent_transfer_history table (id, workflow_id FK, from_agent, to_agent, transfer_reason, context_passed JSONB, created_at)\n   - ALTER workflow_state ADD transfer_count INT DEFAULT 0, ADD current_agent_type VARCHAR(50)\n\n5. WebSocket events: Emit 'agent_transfer' with from/to agents",
      "verificationCriteria": "1. Transfers logged in agent_transfer_history with context preservation\n2. workflow_state.transfer_count increments on each transfer\n3. New agent receives context from previous agent\n4. Maximum 5 transfers enforced per workflow\n5. WebSocket emits 'agent_transfer' events\n6. QualificationAgent can transfer to Enrichment/BDR based on score\n7. Tests cover transfer chains (qual → enrich → bdr)\n8. No circular transfer loops possible",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "3b0df390-4c2b-4269-8acf-e039a4d8a1e7",
      "name": "Phase 2B: Multi-Provider Cerebras Routing",
      "description": "Create unified provider abstraction supporting 4 Cerebras access methods: Direct API, OpenRouter, LangChain, and Cartesia SDK. Implements intelligent fallback cascade and provider selection based on latency/cost constraints.",
      "notes": "CRITICAL: Add to requirements.txt: langchain-cerebras>=0.1.0, langchain-core>=0.1.0. Add to .env: OPENROUTER_API_KEY (already exists). Fallback only if primary fails (circuit breaker open). Log all provider selections for monitoring.",
      "status": "pending",
      "dependencies": [],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "backend/app/services/providers/cerebras_direct.py",
          "type": "CREATE",
          "description": "Direct Cerebras API client"
        },
        {
          "path": "backend/app/services/providers/cerebras_openrouter.py",
          "type": "CREATE",
          "description": "OpenRouter proxy client"
        },
        {
          "path": "backend/app/services/providers/cerebras_langchain.py",
          "type": "CREATE",
          "description": "LangChain wrapper"
        },
        {
          "path": "backend/app/services/providers/provider_selector.py",
          "type": "CREATE",
          "description": "Unified provider manager"
        },
        {
          "path": "backend/app/services/model_router.py",
          "type": "TO_MODIFY",
          "description": "Integrate CerebrasProviderManager"
        },
        {
          "path": "backend/requirements.txt",
          "type": "TO_MODIFY",
          "description": "Add langchain-cerebras, langchain-core"
        }
      ],
      "implementationGuide": "1. Create backend/app/services/providers/ directory:\n\n   cerebras_direct.py:\n   - Uses existing OpenAI SDK with base_url=api.cerebras.ai/v1\n   - Fastest option (633ms baseline)\n   \n   cerebras_openrouter.py:\n   - POST https://openrouter.ai/api/v1/chat/completions\n   - Headers: Authorization Bearer {OPENROUTER_API_KEY}\n   - Body: {\"model\": \"meta-llama/llama-3.3-70b-instruct\", \"provider\": {\"only\": [\"Cerebras\"]}}\n   - Add ~50ms proxy overhead\n   \n   cerebras_langchain.py:\n   - from langchain_cerebras import ChatCerebras\n   - llm = ChatCerebras(model=\"llama-3.3-70b\")\n   - Async: await llm.ainvoke(messages)\n   - Add langchain-cerebras>=0.1.0 to requirements.txt\n   \n   provider_selector.py:\n   - CerebrasProviderManager class\n   - select_provider(constraints: Dict) → str\n   - Fallback cascade: Direct → OpenRouter → LangChain\n   - Logs selection in cerebras_provider_routing table\n\n2. Alembic migration:\n   - cerebras_provider_routing (id, agent_execution_id FK, provider VARCHAR(20), model, reason_for_choice, fallback_occurred BOOLEAN, created_at)\n   - ALTER agent_executions ADD provider_used VARCHAR(20) DEFAULT 'direct'\n\n3. Integration with ModelRouter:\n   - Inject CerebrasProviderManager\n   - Route Cerebras calls through provider selector\n   - Track provider performance metrics",
      "verificationCriteria": "1. All 3 providers (Direct, OpenRouter, LangChain) functional\n2. Fallback cascade works: Direct fails → OpenRouter → LangChain\n3. Provider selection logged in cerebras_provider_routing table\n4. Direct API maintains <700ms latency\n5. OpenRouter adds ~50ms overhead (750ms total)\n6. LangChain async working correctly\n7. Circuit breaker triggers provider fallback\n8. Tests cover all 3 provider paths",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "2bdaf98d-ed83-4876-abcf-b47dcc2e5711",
      "name": "Phase 3: Cartesia Voice Integration",
      "description": "Integrate Cartesia SDK to create real-time voice agents using Cerebras inference. Implements TalkingNode with ReasoningNode pattern, message conversion, and WebSocket streaming for <2s response latency.",
      "notes": "CRITICAL: Cartesia handles STT/TTS automatically. Cerebras provides inference only. Target: <2000ms per voice turn (including STT+LLM+TTS). Use Cerebras llama3.1-8b for 633ms LLM latency. Store full conversation in JSONB for analysis.",
      "status": "pending",
      "dependencies": [
        {
          "taskId": "3b0df390-4c2b-4269-8acf-e039a4d8a1e7"
        }
      ],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "backend/app/services/voice/cartesia_node.py",
          "type": "CREATE",
          "description": "TalkingNode with Cerebras backend"
        },
        {
          "path": "backend/app/services/voice/voice_handler.py",
          "type": "CREATE",
          "description": "Voice session orchestration"
        },
        {
          "path": "backend/app/api/voice.py",
          "type": "CREATE",
          "description": "Voice API endpoints"
        },
        {
          "path": "backend/requirements.txt",
          "type": "TO_MODIFY",
          "description": "Add cartesia-line>=1.0.0"
        },
        {
          "path": ".env",
          "type": "REFERENCE",
          "description": "Add CARTESIA_API_KEY"
        },
        {
          "path": "docker-compose.yml",
          "type": "TO_MODIFY",
          "description": "Add cartesia-agent service"
        }
      ],
      "implementationGuide": "1. Add dependencies to requirements.txt:\n   - cartesia-line>=1.0.0\n   - Add CARTESIA_API_KEY to .env\n\n2. Create backend/app/services/voice/cartesia_node.py:\n   \n   TalkingNode(ReasoningNode):\n   - __init__(system_prompt: str, client: AsyncCerebras)\n   - tools = [end_call_schema] (Cerebras-compatible)\n   - async process_context(context: ConversationContext) → AsyncGenerator\n   - Calls: cs_client.chat.completions.create(messages, tools, stream=False)\n   - Yields: AgentResponse or ToolResult or EndCall\n   \n   convert_messages_to_cs(messages, sys_prompt) → List[Dict]:\n   - Maps AgentResponse → {\"role\": \"assistant\", \"content\": ...}\n   - Maps UserTranscriptionReceived → {\"role\": \"user\", \"content\": ...}\n   - Maps ToolResult → {\"role\": \"system\", \"content\": f\"Tool {name} called\"}\n   \n   end_call_schema:\n   - Cerebras function schema with strict: True\n   - Parameters: goodbye_message (string, required)\n\n3. Create backend/app/services/voice/voice_handler.py:\n   - async handle_new_call(system: VoiceAgentSystem, call_request: CallRequest)\n   - Creates TalkingNode with Cerebras client\n   - Sets up Bridge for event routing\n   - Handles UserStoppedSpeaking with interruption logic\n   - await system.start() and wait_for_shutdown()\n\n4. Alembic migration:\n   - voice_session_logs table (id, lead_id FK, session_id, platform VARCHAR(20), turn_count, total_duration_seconds, stt_provider, tts_provider, avg_latency_ms, total_cost_usd, conversation_transcript JSONB, created_at)\n\n5. API endpoints:\n   - POST /api/voice/create-session → {\"session_id\": str, \"connection_url\": str}\n   - POST /api/voice/end-session/{session_id}\n   - WebSocket /ws/voice/{session_id}\n\n6. Docker integration:\n   - Add cartesia-agent service to docker-compose.yml\n   - Port 8080 for voice WebSocket",
      "verificationCriteria": "1. Voice sessions complete <2000ms per turn (STT+LLM+TTS)\n2. TalkingNode properly converts Cartesia events to Cerebras messages\n3. end_call_schema triggers graceful conversation end\n4. Interruptions handled correctly (UserStartedSpeaking → interrupt)\n5. All turns logged in voice_session_logs with conversation_transcript\n6. WebSocket connection stable throughout call\n7. Costs tracked: Cerebras ($0.000006/turn) + Cartesia (separate)\n8. Tests verify full conversation flow",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "3e255368-68a1-4835-8bee-0d256c84fb43",
      "name": "Phase 3: Document Processing for Resume/Job Matching",
      "description": "Implement PDF processing pipeline to analyze resumes and job postings, extract structured data using Cerebras, and calculate match scores for interview agent use cases.",
      "notes": "CRITICAL: Handle various PDF formats gracefully (text-based and scanned). Use Cerebras structured outputs for reliable JSON extraction. Store raw text + structured JSONB for flexibility. Resume parsing must extract: work history (company, role, duration), education (degree, institution, year), skills (categorized).",
      "status": "pending",
      "dependencies": [],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "backend/app/services/document/pdf_processor.py",
          "type": "CREATE",
          "description": "PDF text extraction"
        },
        {
          "path": "backend/app/services/document/resume_analyzer.py",
          "type": "CREATE",
          "description": "Resume structured extraction"
        },
        {
          "path": "backend/app/services/document/job_matcher.py",
          "type": "CREATE",
          "description": "Resume-job matching algorithm"
        },
        {
          "path": "backend/app/api/documents.py",
          "type": "CREATE",
          "description": "Document API endpoints"
        },
        {
          "path": "backend/requirements.txt",
          "type": "TO_MODIFY",
          "description": "Add PyPDF2, pdfplumber"
        }
      ],
      "implementationGuide": "1. Add dependencies to requirements.txt:\n   - PyPDF2>=3.0.0\n   - pdfplumber>=0.10.0\n\n2. Create backend/app/services/document/pdf_processor.py:\n   - extract_pdf_text(file_path: str) → str\n   - Uses PyPDF2 for basic extraction\n   - Falls back to pdfplumber for complex layouts\n\n3. Create backend/app/services/document/resume_analyzer.py:\n   - analyze_resume(pdf_text: str) → Dict:\n     * Use Cerebras to extract: education, experience, skills, contact\n     * Structured output with JSON schema\n     * Returns: {\"sections\": {\"education\": [...], \"experience\": [...], \"skills\": [...]}, \"years_experience\": int}\n\n4. Create backend/app/services/document/job_matcher.py:\n   - match_resume_to_job(resume_data: Dict, job_posting: str) → float:\n     * Use Cerebras to analyze fit (633ms)\n     * Returns match_score 0.0-1.0\n     * Explanation of match/gaps\n\n5. Alembic migration:\n   - document_analysis table (id, lead_id FK, document_type VARCHAR(50), extracted_text TEXT, structured_data JSONB, match_score FLOAT, created_at)\n\n6. API endpoint:\n   - POST /api/documents/analyze\n   - Body: file (multipart/form-data), lead_id, doc_type\n   - Returns: {\"match_score\": float, \"structured_data\": dict, \"analysis\": str}\n\n7. Integration with ConversationAgent:\n   - Voice agent can query document_analysis table\n   - Use resume insights for interview questions",
      "verificationCriteria": "1. PDF extraction handles text-based and complex layouts\n2. Resume analyzer extracts education, experience, skills accurately\n3. Structured data follows consistent JSON schema\n4. Job matching produces 0.0-1.0 score with explanation\n5. Match scores align with human evaluation (>80% agreement)\n6. All documents stored in document_analysis table\n7. ConversationAgent can access resume insights\n8. API handles file uploads securely (size limits, validation)",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "550fe471-5f1c-4f53-bfef-d59dd40d3274",
      "name": "Phase 4: Frontend Components for New Patterns",
      "description": "Create React components and routes for agent transfers, research pipeline, voice interface, and document analysis with WebSocket streaming and real-time updates.",
      "notes": "Use existing WebSocket infrastructure. Ensure all components are mobile responsive. Add loading states for async operations. Use Tailwind CSS v4 with modern design patterns.",
      "status": "pending",
      "dependencies": [
        {
          "taskId": "09810281-dd8b-439d-9f4d-eb8f98ea43a3"
        },
        {
          "taskId": "adeee671-4c9e-44b9-90c2-f0616c72c1dd"
        },
        {
          "taskId": "2bdaf98d-ed83-4876-abcf-b47dcc2e5711"
        },
        {
          "taskId": "3e255368-68a1-4835-8bee-0d256c84fb43"
        }
      ],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "frontend/src/components/AgentTransferFlow.tsx",
          "type": "CREATE",
          "description": "Transfer flow visualization"
        },
        {
          "path": "frontend/src/components/ResearchPipeline.tsx",
          "type": "CREATE",
          "description": "5-stage pipeline UI"
        },
        {
          "path": "frontend/src/components/VoiceAgent.tsx",
          "type": "CREATE",
          "description": "Voice interface component"
        },
        {
          "path": "frontend/src/components/DocumentAnalyzer.tsx",
          "type": "CREATE",
          "description": "Document upload and analysis"
        },
        {
          "path": "frontend/src/hooks/useAgentTransfer.ts",
          "type": "CREATE",
          "description": "WebSocket transfer events"
        },
        {
          "path": "frontend/src/hooks/useVoiceSession.ts",
          "type": "CREATE",
          "description": "WebSocket voice streaming"
        },
        {
          "path": "frontend/src/App.tsx",
          "type": "TO_MODIFY",
          "description": "Add 5 new routes"
        }
      ],
      "implementationGuide": "1. Create frontend/src/components/AgentTransferFlow.tsx:\n   - Displays transfer history as flow diagram\n   - Shows: from_agent → transfer_reason → to_agent\n   - Highlights current active agent\n   - Uses Tailwind CSS for visual flow\n\n2. Create frontend/src/components/ResearchPipeline.tsx:\n   - 5-stage pipeline visualization\n   - Each stage: QueryGen → WebSearch → Summarize → Synthesize → Format\n   - Progress indicators, article count, current stage highlight\n   - Displays final synthesis with citations\n\n3. Create frontend/src/components/VoiceAgent.tsx:\n   - Platform selector (Cartesia/LiveKit toggle)\n   - Call button with connection status\n   - Real-time transcript display\n   - Waveform visualizer (audio levels)\n   - End call button\n\n4. Create frontend/src/components/DocumentAnalyzer.tsx:\n   - File upload dropzone (accepts .pdf)\n   - Analysis progress indicator\n   - Match score display (0-100% circular progress)\n   - Structured data viewer (education, experience, skills)\n   - Key insights/gaps highlighting\n\n5. Create hooks:\n   - useAgentTransfer.ts (WebSocket for transfer events)\n   - useVoiceSession.ts (WebSocket for voice streaming)\n\n6. Add routes:\n   - /leads/:id/transfers (agent flow)\n   - /leads/:id/research (pipeline viz)\n   - /leads/:id/voice (voice interface)\n   - /leads/:id/documents (doc analysis)\n   - /workflow/:id (workflow overview)",
      "verificationCriteria": "1. AgentTransferFlow displays transfer history correctly\n2. ResearchPipeline shows all 5 stages with progress\n3. VoiceAgent connects to WebSocket and streams audio\n4. DocumentAnalyzer uploads PDFs and displays analysis\n5. All components mobile responsive\n6. WebSocket connections handle disconnects gracefully\n7. Loading states during async operations\n8. Error handling with user-friendly messages",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "1a60051e-c223-4293-9f13-2849dab6ab9e",
      "name": "Phase 5: Testing Infrastructure for New Patterns",
      "description": "Create comprehensive test suites for iterative refinement, agent transfers, research pipeline, voice integration, and document processing. Achieve 90%+ coverage across all new features.",
      "notes": "CRITICAL: All tests must use mocked AI API calls (no real API keys). Use pytest-asyncio for async tests. Mock Exa API, Cartesia SDK, Cerebras API. Set up CI/CD to run all test suites on PR. Fail build if coverage drops below targets.",
      "status": "pending",
      "dependencies": [
        {
          "taskId": "ba4bf981-b5f3-4883-93b5-ea4e24386738"
        },
        {
          "taskId": "09810281-dd8b-439d-9f4d-eb8f98ea43a3"
        },
        {
          "taskId": "adeee671-4c9e-44b9-90c2-f0616c72c1dd"
        },
        {
          "taskId": "2bdaf98d-ed83-4876-abcf-b47dcc2e5711"
        },
        {
          "taskId": "3e255368-68a1-4835-8bee-0d256c84fb43"
        }
      ],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": "backend/tests/test_iterative_refinement.py",
          "type": "CREATE",
          "description": "Refinement test suite (95% coverage)"
        },
        {
          "path": "backend/tests/test_agent_transfers.py",
          "type": "CREATE",
          "description": "Transfer test suite (90% coverage)"
        },
        {
          "path": "backend/tests/test_research_pipeline.py",
          "type": "CREATE",
          "description": "Research test suite (92% coverage)"
        },
        {
          "path": "backend/tests/test_voice_agents.py",
          "type": "CREATE",
          "description": "Voice test suite (85% coverage)"
        },
        {
          "path": "backend/tests/test_document_processing.py",
          "type": "CREATE",
          "description": "Document test suite"
        },
        {
          "path": "backend/tests/test_provider_routing.py",
          "type": "CREATE",
          "description": "Provider routing tests (90% coverage)"
        },
        {
          "path": ".github/workflows/test.yml",
          "type": "TO_MODIFY",
          "description": "Add new test suites to CI/CD"
        }
      ],
      "implementationGuide": "1. Create backend/tests/test_iterative_refinement.py:\n   - test_reflect_step() - Verify key attribute extraction\n   - test_elaborate_step() - Check initial generation\n   - test_critique_step() - Validate quality assessment\n   - test_refine_step() - Ensure final output improved\n   - test_full_refinement_loop() - E2E 4-step flow\n   - test_latency_under_2500ms() - Performance benchmark\n   - test_cost_tracking() - Verify 4x cost calculation\n   - Coverage target: 95%\n\n2. Create backend/tests/test_agent_transfers.py:\n   - test_transfer_from_qualification_to_enrichment()\n   - test_transfer_history_logging()\n   - test_context_preservation_across_transfer()\n   - test_maximum_5_transfers_enforced()\n   - test_circular_transfer_prevention()\n   - Coverage target: 90%\n\n3. Create backend/tests/test_research_pipeline.py:\n   - test_query_generation_from_lead_data()\n   - test_exa_web_search_integration()\n   - test_article_summarization_with_refinement()\n   - test_synthesis_of_multiple_articles()\n   - test_final_formatting_with_citations()\n   - test_pipeline_latency_under_10s()\n   - Coverage target: 92%\n\n4. Create backend/tests/test_voice_agents.py:\n   - test_cartesia_talking_node_initialization()\n   - test_message_conversion_to_cerebras_format()\n   - test_end_call_tool_execution()\n   - test_voice_session_logging()\n   - test_conversation_transcript_storage()\n   - test_voice_latency_under_2000ms()\n   - Coverage target: 85%\n\n5. Create backend/tests/test_document_processing.py:\n   - test_pdf_text_extraction()\n   - test_resume_structured_extraction()\n   - test_job_matching_score_calculation()\n   - test_document_analysis_storage()\n\n6. Create backend/tests/test_provider_routing.py:\n   - test_direct_cerebras_api()\n   - test_openrouter_cerebras_proxy()\n   - test_langchain_cerebras_wrapper()\n   - test_provider_fallback_cascade()\n   - test_provider_selection_logging()\n\n7. Performance benchmarks:\n   - test_refinement_latency_sla() - <2500ms\n   - test_voice_response_sla() - <2000ms\n   - test_research_pipeline_sla() - <10s",
      "verificationCriteria": "1. All test suites pass with required coverage\n2. Iterative refinement: 95% coverage achieved\n3. Agent transfers: 90% coverage achieved\n4. Research pipeline: 92% coverage achieved\n5. Voice agents: 85% coverage achieved\n6. Provider routing: 90% coverage achieved\n7. Performance benchmarks validate SLA compliance\n8. CI/CD fails if coverage drops below thresholds\n9. All tests use mocked APIs (no real API calls)",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    },
    {
      "id": "efdd132b-f6cd-4acf-99e4-7f9fccf09fb1",
      "name": "Phase 5: Deployment and Monitoring Setup",
      "description": "Configure CI/CD pipelines, monitoring dashboards, and production deployment for all new Cerebras patterns. Implement canary rollout strategy and real-time alerting.",
      "notes": "CRITICAL: Canary rollout required for production. Monitor error rates closely during rollout. Have rollback plan ready. Update all documentation before launch. Set up monitoring dashboards BEFORE deployment.",
      "status": "pending",
      "dependencies": [
        {
          "taskId": "1a60051e-c223-4293-9f13-2849dab6ab9e"
        }
      ],
      "createdAt": "2025-10-04T22:00:52.887Z",
      "updatedAt": "2025-10-04T22:00:52.887Z",
      "relatedFiles": [
        {
          "path": ".github/workflows/test.yml",
          "type": "TO_MODIFY",
          "description": "Add new test suites"
        },
        {
          "path": ".github/workflows/deploy.yml",
          "type": "TO_MODIFY",
          "description": "Add deployment steps"
        },
        {
          "path": "backend/app/core/logging.py",
          "type": "TO_MODIFY",
          "description": "Add Datadog/Sentry metrics"
        },
        {
          "path": "README.md",
          "type": "TO_MODIFY",
          "description": "Document all new patterns"
        },
        {
          "path": "CLAUDE.md",
          "type": "TO_MODIFY",
          "description": "Update development guidelines"
        },
        {
          "path": "CEREBRAS_PATTERNS.md",
          "type": "CREATE",
          "description": "Cerebras integration tutorial"
        }
      ],
      "implementationGuide": "1. Update .github/workflows/test.yml:\n   - Add 7 new test suite commands\n   - Fail on coverage drop below thresholds\n   - Run performance benchmarks in CI\n\n2. Update .github/workflows/deploy.yml:\n   - Add docker-compose up -d cartesia-agent\n   - Run Alembic migrations: alembic upgrade head\n   - Canary deployment: 10% → 50% → 100% traffic\n   - Rollback on error threshold (>5% error rate)\n\n3. Datadog Custom Metrics:\n   - datadog.increment('refinement.iterations', tags=['step:reflect/elaborate/critique/refine'])\n   - datadog.histogram('refinement.latency_ms')\n   - datadog.increment('agent.transfer', tags=['from:qual', 'to:enrich'])\n   - datadog.histogram('research_pipeline.duration_ms')\n   - datadog.histogram('voice.latency_ms')\n   - datadog.increment('provider.selection', tags=['provider:direct/openrouter/langchain'])\n\n4. Sentry Error Tracking:\n   - sentry.capture_message('Agent transfer', level='info', extra={...})\n   - sentry.capture_exception(refinement_error)\n\n5. Alerting Rules:\n   - Alert if refinement latency >3000ms (20% above SLA)\n   - Alert if voice latency >2400ms (20% above SLA)\n   - Alert if research pipeline >12s (20% above SLA)\n   - Alert if cost exceeds $600/month (20% above target)\n   - Alert if error rate >5% for any pattern\n\n6. Documentation Updates:\n   - Update README.md with all new patterns\n   - Update CLAUDE.md with implementation details\n   - Create CEREBRAS_PATTERNS.md tutorial\n\n7. Database Backup Strategy:\n   - Backup production DB before migrations\n   - Test migrations on staging first\n   - Alembic rollback scripts for each migration",
      "verificationCriteria": "1. CI/CD runs all 7 test suites successfully\n2. Canary deployment progresses: 10% → 50% → 100%\n3. Datadog dashboards show all custom metrics\n4. Sentry captures errors from all new patterns\n5. Alerting rules trigger on SLA violations\n6. README.md reflects all new capabilities\n7. CEREBRAS_PATTERNS.md tutorial complete\n8. Database backups working before migrations\n9. Rollback procedures tested and documented",
      "analysisResult": "CEREBRAS MULTI-AGENT PATTERNS INTEGRATION\n\n**PROJECT GOAL**: Integrate 4 advanced Cerebras patterns into sales-agent platform to create production-ready multi-agent system with iterative refinement, research pipeline, agent transfers, and voice capabilities.\n\n**TECHNICAL FOUNDATION**:\n- Current: FastAPI + BaseAgent pattern + ModelRouter + WebSocket streaming\n- Cerebras: 633ms latency (39% under 1000ms target), $0.000006 per call\n- Architecture: 4 AI providers (Cerebras, Claude, DeepSeek, Ollama) with circuit breakers\n\n**KEY PATTERNS TO INTEGRATE**:\n1. Iterative Refinement: 4-step loop (reflect → elaborate → critique → refine) for quality\n2. Research Pipeline: 5-agent assembly (QueryGen → WebSearch → Summarize → Synthesize → Format)\n3. Agent Transfers: Function-based handoffs using @transfer_tool decorator\n4. Voice Integration: Cartesia SDK + optional LiveKit for real-time conversations\n5. Document Processing: PDF analysis for resume/job matching\n6. Multi-Provider: Direct/OpenRouter/LangChain Cerebras access\n\n**PERFORMANCE TARGETS**:\n- Iterative refinement: <2500ms total latency\n- Voice response: <2000ms per turn\n- Research pipeline: <10s end-to-end\n- Monthly cost: <$500 for 10K leads\n\n**INTEGRATION STRATEGY**:\n- Extend BaseAgent with new methods (iterate_refine, transfer_to, get_transfer_tools)\n- Enhance ModelRouter with provider abstraction (CerebrasProviderManager)\n- Add 6 new database tables via Alembic migrations\n- Create 35+ new files across backend/frontend\n- Maintain backward compatibility with existing endpoints"
    }
  ]
}